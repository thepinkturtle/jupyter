{
  "nbformat_minor": 1,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AutoEncoders\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Importing the dataset\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "# Preparing the training set and the test set\n",
        "training_set = np.array(pd.read_csv('ml-100k/u1.base', delimiter='\\t'), dtype=int)\n",
        "test_set = np.array(pd.read_csv('ml-100k/u1.test', delimiter='\\t'), dtype=int)\n",
        "\n",
        "# Getting the number of users and movies\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0]))) # must check both traing and test set for max number\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))# must check both traing and test set for max number\n",
        "\n",
        "\n",
        "# create the matrix of users = lines/rows and movies = col/features\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1,nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users ] # get all the movies by the user_id\n",
        "        id_rating = data[:,2][data[:,0] == id_users ] # get all the rating by the user_id\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_rating\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "\n",
        "# convert and change to Torch Tensors\n",
        "training_set = torch.FloatTensor(convert(training_set))\n",
        "test_set = torch.FloatTensor(convert(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the architecture of the neural network\n",
        "class SAE(nn.Module): # inheritance\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__() # use this to access the inherited functions\n",
        "        self.fc1 = nn.Linear(nb_movies, 20) # first full connection: input features, nodes in layer\n",
        "        self.fc2 = nn.Linear(20, 10) # second layer input is eqaul to first layer nodes\n",
        "        self.fc3 = nn.Linear(10, 20) # third layer start to decode\n",
        "        self.fc4 = nn.Linear(20, nb_movies) # output layer same number of outputs as original inputs\n",
        "        self.activation = nn.Sigmoid() # activation function\n",
        "    def forward(self, x): # encoding\n",
        "        x = self.activation(self.fc1(x)) \n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x) # don't need to use activation function on output layer\n",
        "        return x\n",
        "\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr= 0.01, weight_decay=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "epoch:  1  loss:  1.7598484071728857\nepoch:  2  loss:  1.0962776981164422\nepoch:  3  loss:  1.0535493613959406\nepoch:  4  loss:  1.038243894273267\nepoch:  5  loss:  1.0309641064925403\nepoch:  6  loss:  1.0265935410882168\nepoch:  7  loss:  1.0237039728830963\nepoch:  8  loss:  1.0221266222421563\nepoch:  9  loss:  1.0207291565560141\nepoch:  10  loss:  1.0197919737482286\nepoch:  11  loss:  1.0189784572233325\nepoch:  12  loss:  1.0184690323302705\nepoch:  13  loss:  1.0177094236231943\nepoch:  14  loss:  1.0176259127786826\nepoch:  15  loss:  1.0173172179765642\nepoch:  16  loss:  1.0169021940279352\nepoch:  17  loss:  1.0168245627895258\nepoch:  18  loss:  1.0164545832905074\nepoch:  19  loss:  1.016537269331242\nepoch:  20  loss:  1.016091363820186\nepoch:  21  loss:  1.0160634366032077\nepoch:  22  loss:  1.0158503001373378\nepoch:  23  loss:  1.0157894622157873\nepoch:  24  loss:  1.0158709087163682\nepoch:  25  loss:  1.0155187620737038\nepoch:  26  loss:  1.0156708999178314\nepoch:  27  loss:  1.0154199842738028\nepoch:  28  loss:  1.0150790627089354\nepoch:  29  loss:  1.012812739684439\nepoch:  30  loss:  1.0112876730741054\nepoch:  31  loss:  1.0097234657151466\nepoch:  32  loss:  1.0074471940928758\nepoch:  33  loss:  1.0079871571444972\nepoch:  34  loss:  1.0030877826303621\nepoch:  35  loss:  1.0026248365200219\nepoch:  36  loss:  1.0001968170381463\nepoch:  37  loss:  0.9991813785831412\nepoch:  38  loss:  0.9974903173816999\nepoch:  39  loss:  0.9957494539954773\nepoch:  40  loss:  0.9929869233825018\nepoch:  41  loss:  0.9925921217619673\nepoch:  42  loss:  0.989515520639926\nepoch:  43  loss:  0.9908901463661057\nepoch:  44  loss:  0.9864842484720272\nepoch:  45  loss:  0.983682497197748\nepoch:  46  loss:  0.9817775411470518\nepoch:  47  loss:  0.9895196644643587\nepoch:  48  loss:  0.9850672304081542\nepoch:  49  loss:  0.9850053526394799\nepoch:  50  loss:  0.9829100988399272\nepoch:  51  loss:  0.9834555298252559\nepoch:  52  loss:  0.9779125022849083\nepoch:  53  loss:  0.9747607228070683\nepoch:  54  loss:  0.9719590367196164\nepoch:  55  loss:  0.9732750240190949\nepoch:  56  loss:  0.9697799386532685\nepoch:  57  loss:  0.9755458629417857\nepoch:  58  loss:  0.9684621430377306\nepoch:  59  loss:  0.9696043125885965\nepoch:  60  loss:  0.969712304270546\nepoch:  61  loss:  0.9691711193719712\nepoch:  62  loss:  0.9663111863796062\nepoch:  63  loss:  0.9735866447401335\nepoch:  64  loss:  0.978464598270831\nepoch:  65  loss:  0.9737009158031038\nepoch:  66  loss:  0.9717450752448887\nepoch:  67  loss:  0.9690896337185484\nepoch:  68  loss:  0.9678235833611125\nepoch:  69  loss:  0.9656787034793891\nepoch:  70  loss:  0.9654308528113762\nepoch:  71  loss:  0.964709358276965\nepoch:  72  loss:  0.9601824448733647\nepoch:  73  loss:  0.9594711354601138\nepoch:  74  loss:  0.9591032771431266\nepoch:  75  loss:  0.9588198393084012\nepoch:  76  loss:  0.9582149367654792\nepoch:  77  loss:  0.9562490382300811\nepoch:  78  loss:  0.9540815212967736\nepoch:  79  loss:  0.9548296955052374\nepoch:  80  loss:  0.9534602075860792\nepoch:  81  loss:  0.95190837239716\nepoch:  82  loss:  0.9509474280410481\nepoch:  83  loss:  0.9510153916310379\nepoch:  84  loss:  0.9489328802003358\nepoch:  85  loss:  0.9479629780062949\nepoch:  86  loss:  0.9468192699987309\nepoch:  87  loss:  0.9462729478148141\nepoch:  88  loss:  0.9453085181582755\nepoch:  89  loss:  0.9465936224592819\nepoch:  90  loss:  0.9458100405877553\nepoch:  91  loss:  0.9437836560867768\nepoch:  92  loss:  0.943274346417506\nepoch:  93  loss:  0.942913652158898\nepoch:  94  loss:  0.9417494360588223\nepoch:  95  loss:  0.9416323311513298\nepoch:  96  loss:  0.9416032045859928\nepoch:  97  loss:  0.941270739170438\nepoch:  98  loss:  0.9406726354447961\nepoch:  99  loss:  0.9397212167095251\nepoch:  100  loss:  0.9393456419015335\nepoch:  101  loss:  0.9386105262402865\nepoch:  102  loss:  0.9381238778414708\nepoch:  103  loss:  0.9376570577298667\nepoch:  104  loss:  0.9400802351686733\nepoch:  105  loss:  0.9369546792975665\nepoch:  106  loss:  0.9380883303185525\nepoch:  107  loss:  0.935628891448092\nepoch:  108  loss:  0.9351145850947674\nepoch:  109  loss:  0.9349593763202627\nepoch:  110  loss:  0.9350418151310551\nepoch:  111  loss:  0.934234312584819\nepoch:  112  loss:  0.9339906359512627\nepoch:  113  loss:  0.9346588971676753\nepoch:  114  loss:  0.9370878297945537\nepoch:  115  loss:  0.9401821610075077\nepoch:  116  loss:  0.9373428319651854\nepoch:  117  loss:  0.9378962778910944\nepoch:  118  loss:  0.9356980949244721\nepoch:  119  loss:  0.9370510182847543\nepoch:  120  loss:  0.9349426992521677\nepoch:  121  loss:  0.9355782057730098\nepoch:  122  loss:  0.9341022636183403\nepoch:  123  loss:  0.934823918311393\nepoch:  124  loss:  0.9332080438763094\nepoch:  125  loss:  0.9338362214149516\nepoch:  126  loss:  0.9321901960413017\nepoch:  127  loss:  0.9333756112711028\nepoch:  128  loss:  0.9326371485920232\nepoch:  129  loss:  0.9324896452325435\nepoch:  130  loss:  0.9310785966389259\nepoch:  131  loss:  0.9313246170477808\nepoch:  132  loss:  0.930565372902592\nepoch:  133  loss:  0.9308193789026386\nepoch:  134  loss:  0.9298649854552682\nepoch:  135  loss:  0.9303460025024468\nepoch:  136  loss:  0.9290566144147999\nepoch:  137  loss:  0.9297742283366918\nepoch:  138  loss:  0.9288056548637426\nepoch:  139  loss:  0.929513078381204\nepoch:  140  loss:  0.9284508382172676\nepoch:  141  loss:  0.9289637340755258\nepoch:  142  loss:  0.9273004734506363\nepoch:  143  loss:  0.9316730990409149\nepoch:  144  loss:  0.9340898392200793\nepoch:  145  loss:  0.9322313547220209\nepoch:  146  loss:  0.9313647165404975\nepoch:  147  loss:  0.9312685938520645\nepoch:  148  loss:  0.930517067825808\nepoch:  149  loss:  0.9298773068738239\nepoch:  150  loss:  0.9309318115906754\nepoch:  151  loss:  0.931915042186061\nepoch:  152  loss:  0.9293140865975053\nepoch:  153  loss:  0.9284169411013523\nepoch:  154  loss:  0.9284251431035979\nepoch:  155  loss:  0.9265963735968902\nepoch:  156  loss:  0.9262146659424477\nepoch:  157  loss:  0.9262117723870055\nepoch:  158  loss:  0.925319225633988\nepoch:  159  loss:  0.9246436941200783\nepoch:  160  loss:  0.9244456580712044\nepoch:  161  loss:  0.9253431890269367\nepoch:  162  loss:  0.9255430274345452\nepoch:  163  loss:  0.9240236972786745\nepoch:  164  loss:  0.9240584671246597\nepoch:  165  loss:  0.9234228253246947\nepoch:  166  loss:  0.9224082489220401\nepoch:  167  loss:  0.9270608227553016\nepoch:  168  loss:  0.9299162483541049\nepoch:  169  loss:  0.9274987281914205\nepoch:  170  loss:  0.9292030289051088\nepoch:  171  loss:  0.9255384983816358\nepoch:  172  loss:  0.9244711216451214\nepoch:  173  loss:  0.9238353419750136\nepoch:  174  loss:  0.923574015532303\nepoch:  175  loss:  0.9274846259883861\nepoch:  176  loss:  0.9251509597369457\nepoch:  177  loss:  0.9261196068248204\nepoch:  178  loss:  0.9233241427720171\nepoch:  179  loss:  0.9247942653129556\nepoch:  180  loss:  0.9223644298492507\nepoch:  181  loss:  0.9213883341589707\nepoch:  182  loss:  0.920072302771704\nepoch:  183  loss:  0.9201620698580137\nepoch:  184  loss:  0.920147209256161\nepoch:  185  loss:  0.9203054121730045\nepoch:  186  loss:  0.9194414188277511\nepoch:  187  loss:  0.9204382332582076\nepoch:  188  loss:  0.9180684973960421\nepoch:  189  loss:  0.9180767710496087\nepoch:  190  loss:  0.9183651092017876\nepoch:  191  loss:  0.9188769665369096\nepoch:  192  loss:  0.9181868374827601\nepoch:  193  loss:  0.918153623630682\nepoch:  194  loss:  0.9171128693996696\nepoch:  195  loss:  0.9166433608353515\nepoch:  196  loss:  0.9162558462206567\nepoch:  197  loss:  0.9163987617379998\nepoch:  198  loss:  0.9160899505630867\nepoch:  199  loss:  0.9160707486072813\nepoch:  200  loss:  0.915601802114744\n"
        }
      ],
      "source": [
        "# Training the SAE optimized\n",
        "\n",
        "nb_epoch = 200\n",
        "\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "       # create batch of single vector online learning, update weights on every input vector\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        # optimization technique only look at users that have rated the movie\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input) # make prediction\n",
        "            target.require_grad = False # don't compute gradient, optimization technique\n",
        "            output[target == 0] = 0 # values won't count in the computation of error :. optimize by not using them\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # stop denominator from being 0\n",
        "            loss.backward() # decided to increase of decrease\n",
        "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step() # decides the intensity of the increase of decrease\n",
        "    print('epoch: ', str(epoch), \" loss: \", str(train_loss/s))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/home/devinlax/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1682])) that is different to the input size (torch.Size([1, 1682])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\ntest loss:  0.9526427978769891\n"
        }
      ],
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "     # create batch of single vector online learning, update weights on every input vector\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user])\n",
        "    # optimization technique only look at users that have rated the movie\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input) # make prediction\n",
        "        target.require_grad = False # don't compute gradient, optimization technique\n",
        "        output[(target == 0).unsqueeze(0)] = 0 # values won't count in the computation of error :. optimize by not using them\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # stop denominator from being 0\n",
        "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
        "        s += 1.\n",
        "print(\"test loss: \", str(test_loss/s))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.6.8-final",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "anaconda-cloud": {}
  }
}