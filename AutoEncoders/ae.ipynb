{
  "nbformat_minor": 1,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AutoEncoders\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Importing the dataset\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "# Preparing the training set and the test set\n",
        "training_set = np.array(pd.read_csv('ml-100k/u1.base', delimiter='\\t'), dtype=int)\n",
        "test_set = np.array(pd.read_csv('ml-100k/u1.test', delimiter='\\t'), dtype=int)\n",
        "\n",
        "# Getting the number of users and movies\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0]))) # must check both traing and test set for max number\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))# must check both traing and test set for max number\n",
        "\n",
        "\n",
        "# create the matrix of users = lines/rows and movies = col/features\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1,nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users ] # get all the movies by the user_id\n",
        "        id_rating = data[:,2][data[:,0] == id_users ] # get all the rating by the user_id\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_rating\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "\n",
        "# convert and change to Torch Tensors\n",
        "training_set = torch.FloatTensor(convert(training_set))\n",
        "test_set = torch.FloatTensor(convert(test_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create the architecture of the neural network\n",
        "class SAE(nn.Module): # inheritance\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__() # use this to access the inherited functions\n",
        "        self.fc1 = nn.Linear(nb_movies, 20) # first full connection: input features, nodes in layer\n",
        "        self.fc2 = nn.Linear(20, 10) # second layer input is eqaul to first layer nodes\n",
        "        self.fc3 = nn.Linear(10, 20) # third layer start to decode\n",
        "        self.fc4 = nn.Linear(20, nb_movies) # output layer same number of outputs as original inputs\n",
        "        self.activation = nn.Sigmoid() # activation function\n",
        "    def forward(self, x): # encoding\n",
        "        x = self.activation(self.fc1(x)) \n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x) # don't need to use activation function on output layer\n",
        "        return x\n",
        "\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr= 0.01, weight_decay=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "epoch:  1  loss:  1.7598484071728857\nepoch:  2  loss:  1.0962776981164422\nepoch:  3  loss:  1.0535493613959406\nepoch:  4  loss:  1.038243894273267\nepoch:  5  loss:  1.0309641064925403\nepoch:  6  loss:  1.0265935410882168\nepoch:  7  loss:  1.0237039728830963\nepoch:  8  loss:  1.0221266222421563\nepoch:  9  loss:  1.0207291565560141\nepoch:  10  loss:  1.0197919737482286\nepoch:  11  loss:  1.0189784572233325\nepoch:  12  loss:  1.0184690323302705\nepoch:  13  loss:  1.0177094236231943\nepoch:  14  loss:  1.0176259127786826\nepoch:  15  loss:  1.0173172179765642\nepoch:  16  loss:  1.0169021940279352\nepoch:  17  loss:  1.0168245627895258\nepoch:  18  loss:  1.0164545832905074\nepoch:  19  loss:  1.016537269331242\nepoch:  20  loss:  1.016091363820186\nepoch:  21  loss:  1.0160634366032077\nepoch:  22  loss:  1.0158503001373378\nepoch:  23  loss:  1.0157894622157873\nepoch:  24  loss:  1.0158709087163682\nepoch:  25  loss:  1.0155187620737038\nepoch:  26  loss:  1.0156708999178314\nepoch:  27  loss:  1.0154199842738028\nepoch:  28  loss:  1.0150790627089354\nepoch:  29  loss:  1.012812739684439\nepoch:  30  loss:  1.0112876730741054\nepoch:  31  loss:  1.0097234657151466\nepoch:  32  loss:  1.0074471940928758\nepoch:  33  loss:  1.0079871571444972\nepoch:  34  loss:  1.0030877826303621\nepoch:  35  loss:  1.0026248365200219\nepoch:  36  loss:  1.0001968170381463\nepoch:  37  loss:  0.9991813785831412\nepoch:  38  loss:  0.9974903173816999\nepoch:  39  loss:  0.9957494539954773\nepoch:  40  loss:  0.9929869233825018\nepoch:  41  loss:  0.9925921217619673\nepoch:  42  loss:  0.989515520639926\nepoch:  43  loss:  0.9908901463661057\nepoch:  44  loss:  0.9864842484720272\nepoch:  45  loss:  0.983682497197748\nepoch:  46  loss:  0.9817775411470518\nepoch:  47  loss:  0.9895196644643587\nepoch:  48  loss:  0.9850672304081542\nepoch:  49  loss:  0.9850053526394799\nepoch:  50  loss:  0.9829100988399272\nepoch:  51  loss:  0.9834555298252559\nepoch:  52  loss:  0.9779125022849083\nepoch:  53  loss:  0.9747607228070683\nepoch:  54  loss:  0.9719590367196164\nepoch:  55  loss:  0.9732750240190949\nepoch:  56  loss:  0.9697799386532685\nepoch:  57  loss:  0.9755458629417857\nepoch:  58  loss:  0.9684621430377306\nepoch:  59  loss:  0.9696043125885965\nepoch:  60  loss:  0.969712304270546\nepoch:  61  loss:  0.9691711193719712\nepoch:  62  loss:  0.9663111863796062\nepoch:  63  loss:  0.9735866447401335\nepoch:  64  loss:  0.978464598270831\nepoch:  65  loss:  0.9737009158031038\nepoch:  66  loss:  0.9717450752448887\nepoch:  67  loss:  0.9690896337185484\nepoch:  68  loss:  0.9678235833611125\nepoch:  69  loss:  0.9656787034793891\nepoch:  70  loss:  0.9654308528113762\nepoch:  71  loss:  0.964709358276965\nepoch:  72  loss:  0.9601824448733647\nepoch:  73  loss:  0.9594711354601138\nepoch:  74  loss:  0.9591032771431266\nepoch:  75  loss:  0.9588198393084012\n"
        }
      ],
      "source": [
        "# Training the SAE optimized\n",
        "\n",
        "nb_epoch = 200\n",
        "\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "       # create batch of single vector online learning, update weights on every input vector\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        # optimization technique only look at users that have rated the movie\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input) # make prediction\n",
        "            target.require_grad = False # don't compute gradient, optimization technique\n",
        "            output[target == 0] = 0 # values won't count in the computation of error :. optimize by not using them\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # stop denominator from being 0\n",
        "            loss.backward() # decided to increase of decrease\n",
        "            train_loss += np.sqrt(loss.item() * mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step() # decides the intensity of the increase of decrease\n",
        "    print('epoch: ', str(epoch), \" loss: \", str(train_loss/s))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "     # create batch of single vector online learning, update weights on every input vector\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user])\n",
        "    # optimization technique only look at users that have rated the movie\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input) # make prediction\n",
        "        target.require_grad = False # don't compute gradient, optimization technique\n",
        "        output[target == 0] = 0 # values won't count in the computation of error :. optimize by not using them\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # stop denominator from being 0\n",
        "        test_loss += np.sqrt(loss.item() * mean_corrector)\n",
        "        s += 1.\n",
        "print(\"test loss: \", str(test_loss/s))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.6.8-final",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "anaconda-cloud": {}
  }
}